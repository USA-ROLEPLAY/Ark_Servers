name: ASA Cluster Compose Deploy

on:
  push:
    branches: [ main ]
    paths:
      - 'ASA/**/docker-compose.yaml'
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

# Do NOT serialize by host at the GitHub level (that caused cancellations).
# We serialize per host on the REMOTE with flock.
concurrency:
  group: asa-cluster-deploy-${{ github.run_id }}
  cancel-in-progress: false

env:
  # Prod waits
  WAIT_30M: "1800"
  WAIT_15M: "900"
  WAIT_3M:  "180"

  LOG_READY_TIMEOUT:  "600"    # 10m
  RCON_READY_TIMEOUT: "1200"   # 20m

jobs:
  build-matrix:
    runs-on: self-hosted
    outputs:
      matrix: ${{ steps.mk-matrix.outputs.matrix }}
    steps:
      - name: Checkout
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Changed files (compose only)
        id: changed
        uses: tj-actions/changed-files@v45
        with:
          files: |
            ASA/**/docker-compose.yaml
          separator: "\n"

      - name: Build matrix from deploy/targets.json
        id: mk-matrix
        uses: actions/github-script@v7
        env:
          CHANGED: ${{ steps.changed.outputs.all_changed_files }}
        with:
          script: |
            const fs = require('fs');
            const changed = (process.env.CHANGED || '')
              .split('\n')
              .map(s => s.trim())
              .filter(Boolean);
            const manifest = JSON.parse(fs.readFileSync('deploy/targets.json','utf8'));
            const picks = [];
            for (const t of manifest.targets) {
              const composePath = `${t.work_subdir.replace(/\/+$/,'')}/docker-compose.yaml`;
              if (changed.includes(composePath)) picks.push(t);
            }
            // De-dupe by id
            const uniq = Object.values(Object.fromEntries(picks.map(t => [t.id, t])));
            core.info(`Changed compose files: ${changed.join(', ') || '(none)'}`);
            core.info(`Selected maps: ${uniq.map(t => t.id).join(', ') || '(none)'}`);
            core.setOutput('matrix', JSON.stringify(uniq));

  deploy:
    needs: build-matrix
    if: ${{ needs.build-matrix.outputs.matrix != '[]' }}
    runs-on: self-hosted

    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJSON(needs.build-matrix.outputs.matrix) }}

    env:
      SSH_USER: ${{ secrets.ASA_SERVER_SSH_USER }}
      SSH_PORT: ${{ secrets.ASA_SERVER_PORT }}
      HOST: ${{ secrets[matrix.ssh_host_secret] }}
      WORK_DIR: ${{ secrets.WORK_DIR }}

      REPO_SUBDIR: ${{ matrix.work_subdir }}
      CNAME: ${{ matrix.container }}
      LIVE_WIN_DIR: /var/lib/docker/volumes/${{ matrix.volume }}/_data/ShooterGame/Saved/Config/WindowsServer
      REPO_INI_DIR: ${{ github.workspace }}/${{ matrix.work_subdir }}
      LIVE_TMP: live-${{ matrix.id }}

    steps:
      - name: Mint GitHub App token
        id: app
        uses: actions/create-github-app-token@v2
        with:
          app-id: ${{ secrets.APP_ID }}
          private-key: ${{ secrets.APP_PRIVATE_KEY }}

      - name: Checkout (for PR diffs)
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ steps.app.outputs.token }}

      - name: Setup SSH
        uses: webfactory/ssh-agent@v0.9.1
        with:
          ssh-private-key: ${{ secrets.ASA_SERVERS_PRIVATE_KEY }}

      # -------- Drain & stop (remote host lock protects per-host restart) --------
      - name: RCON 30m warning
        run: |
          ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
            "sudo docker exec '$CNAME' asa-ctrl rcon --exec \"serverchat Server restarting in 30 minutes for updates. Get somewhere safe.\" || true"

      - name: Wait 15m
        run: sleep ${{ env.WAIT_30M }}

      - name: RCON 15m warning
        run: |
          ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
            "sudo docker exec '$CNAME' asa-ctrl rcon --exec \"serverchat Server restarting in 15 minutes for updates. Get somewhere safe.\" || true"

      - name: Wait 12m
        run: sleep ${{ env.WAIT_15M }}

      - name: RCON 3m warning
        run: |
          ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
            "sudo docker exec '$CNAME' asa-ctrl rcon --exec \"serverchat Server restarting in 3 minutes for updates. Final Warning get somewhere safe.\" || true"

      - name: Wait 3m
        run: sleep ${{ env.WAIT_3M }}

      - name: Saveworld & shutdown announce
        run: |
          ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
            "sudo docker exec '$CNAME' asa-ctrl rcon --exec \"serverchat Saving world and shutting down for updates.\" && \
             sudo docker exec '$CNAME' asa-ctrl rcon --exec \"saveworld\" || true"

      # -------- Everything below runs under a per-host remote lock --------
      - name: Stop, update, start (under remote host lock)
        run: |
          # We pass args to the remote bash so we don't rely on env expansion in heredocs.
          ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" 'bash -se -s --' "$WORK_DIR" "$REPO_SUBDIR" "$CNAME" "${{ steps.app.outputs.token }}" "${{ github.repository }}" <<'EOS'
          set -euo pipefail
          WORK_DIR="$1"; SUBDIR="$2"; CNAME="$3"; TOKEN="$4"; REPO_SLUG="$5"

          # Acquire a host-wide lock (blocks other map jobs on this host until released)
          exec 9>/tmp/asa-deploy.lock
          flock -w 7200 9

          cd "$WORK_DIR/$SUBDIR"
          # Stop
          sudo docker compose down

          # Update on-host checkout (compose only)
          REPO_URL="https://x-access-token:${TOKEN}@github.com/${REPO_SLUG}.git"
          GIT_TERMINAL_PROMPT=0 git fetch --no-tags --prune "$REPO_URL" main
          git reset --hard FETCH_HEAD

          # Start
          sudo docker compose up -d
          EOS

      - name: Wait for startup log
        run: |
          ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" 'bash -se -s --' "$CNAME" "$LOG_READY_TIMEOUT" <<'EOS'
          set -euo pipefail
          CNAME="$1"; TIMEOUT="$2"
          end=$((SECONDS+TIMEOUT))
          while ! sudo docker logs --tail 500 "$CNAME" 2>&1 | grep -q "Starting the ARK: Survival Ascended dedicated server"; do
            if [ $SECONDS -gt $end ]; then
              echo "Timed out waiting for startup log ($CNAME)" >&2
              sudo docker logs "$CNAME" --tail 200 || true
              exit 1
            fi
            sleep 5
          done
          EOS

      - name: Wait for RCON to respond
        run: |
          ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" 'bash -se -s --' "$CNAME" "$RCON_READY_TIMEOUT" <<'EOS'
          set -euo pipefail
          CNAME="$1"; TIMEOUT="$2"
          end=$((SECONDS+TIMEOUT))
          while :; do
            if sudo docker exec "$CNAME" asa-ctrl rcon --exec "listplayers" >/dev/null 2>&1 \
               || sudo docker exec "$CNAME" asa-ctrl rcon --exec "serverchat healthcheck" >/dev/null 2>&1; then
              break
            fi
            if [ $SECONDS -gt $end ]; then
              echo "Timed out waiting for RCON ($CNAME)" >&2
              sudo docker logs "$CNAME" --tail 200 || true
              exit 1
            fi
            sleep 5
          done
          EOS

      # -------- Live -> repo PR (ONLY GameUserSettings.ini) --------
      - name: Fetch live GameUserSettings.ini (read-only mount)
        run: |
          mkdir -p "$LIVE_TMP"
          ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
            "sudo docker run --rm -v '${{ matrix.volume }}:/v:ro' alpine:3.19 \
             sh -c 'if [ -f /v/ShooterGame/Saved/Config/WindowsServer/GameUserSettings.ini ]; then cat /v/ShooterGame/Saved/Config/WindowsServer/GameUserSettings.ini; fi'" \
            > "$LIVE_TMP/GameUserSettings.ini" || true

      - name: Scrub admin password placeholder
        run: |
          if [ -s "$LIVE_TMP/GameUserSettings.ini" ]; then
            sed -i 's|^ServerAdminPassword=.*$|ServerAdminPassword=\${SERVER_PASSWORD}|' "$LIVE_TMP/GameUserSettings.ini"
          fi

      - name: Diff & stage GameUserSettings.ini
        id: stage
        run: |
          set -euo pipefail
          mkdir -p "$REPO_INI_DIR"
          changed=0
          if [ -s "$LIVE_TMP/GameUserSettings.ini" ]; then
            if ! diff -q "$REPO_INI_DIR/GameUserSettings.ini" "$LIVE_TMP/GameUserSettings.ini" >/dev/null 2>&1; then
              cp "$LIVE_TMP/GameUserSettings.ini" "$REPO_INI_DIR/GameUserSettings.ini"
              echo "updated GameUserSettings.ini"
              changed=1
            fi
          fi
          echo "changed=$changed" >> "$GITHUB_OUTPUT"

      - name: Create PR with updated GameUserSettings.ini
        if: steps.stage.outputs.changed == '1'
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ steps.app.outputs.token }}
          commit-message: "chore(${{ matrix.id }}): sync live GameUserSettings.ini -> repo"
          title: "Sync GameUserSettings.ini from live: ${{ matrix.id }}"
          body: |
            Syncing `GameUserSettings.ini` from the live server after restart.
            - Target: **${{ matrix.id }}**
          branch: chore/sync-gus/${{ matrix.id }}-${{ github.run_id }}
          delete-branch: true
          add-paths: |
            ${{ matrix.work_subdir }}/GameUserSettings.ini

      - name: Summarize
        run: |
          if [ "${{ steps.stage.outputs.changed }}" = "1" ]; then
            echo "### GUS changed → PR opened" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "### No GUS changes detected → no PR" >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Kill ssh-agent (cleanup)
        if: always()
        run: |
          if [ -n "${SSH_AGENT_PID:-}" ]; then ssh-agent -k || true; fi
