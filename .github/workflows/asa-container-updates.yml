name: ASA Cluster Compose Deploy

on:
  push:
    branches: [ main ]
    paths:
      - 'ASA/**/docker-compose.yaml'
  workflow_dispatch:

concurrency:
  # One slot per physical host across all runs (prevents overlapping restarts on the same box)
  group: asa-cluster-deploy
  cancel-in-progress: false

permissions:
  contents: write
  pull-requests: write

env:
  # Prod waits (you can temporarily override in the UI for testing)
  WAIT_30M: "1800"
  WAIT_15M: "900"
  WAIT_3M:  "180"

  LOG_READY_TIMEOUT:  "600"    # 10m
  RCON_READY_TIMEOUT: "1200"   # 20m

jobs:
  build-matrix:
    runs-on: self-hosted
    outputs:
      matrix: ${{ steps.mk-matrix.outputs.matrix }}
    steps:
      - name: Checkout
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Changed files (compose only)
        id: changed
        uses: tj-actions/changed-files@v45
        with:
          files: |
            ASA/**/docker-compose.yaml
          separator: "\n"

      - name: Build host-grouped matrix
        id: mk-matrix
        uses: actions/github-script@v7
        env:
          CHANGED: ${{ steps.changed.outputs.all_changed_files }}
        with:
          script: |
            const fs = require('fs');

            // manifest: targets.json -> [{ id, work_subdir, container, volume, ssh_host_secret }, ...]
            const manifest = JSON.parse(fs.readFileSync('deploy/targets.json','utf8'));

            const changedList = (process.env.CHANGED || '')
              .split('\n')
              .map(s => s.trim())
              .filter(Boolean); // exact changed compose paths

            // group by host secret
            const byHost = new Map();
            for (const t of manifest.targets) {
              const composePath = `${t.work_subdir.replace(/\/+$/,'')}/docker-compose.yaml`;
              if (!changedList.includes(composePath)) continue;
              const key = t.ssh_host_secret;
              if (!byHost.has(key)) byHost.set(key, []);
              byHost.get(key).push({
                id: t.id,
                work_subdir: t.work_subdir,
                container: t.container,
                volume: t.volume
              });
            }

            // matrix entries: one per host
            const matrix = [...byHost.entries()].map(([ssh_host_secret, targets]) => ({
              ssh_host_secret,
              targets
            }));

            core.info(`Changed compose files: ${changedList.join(', ') || '(none)'}`);
            core.info(`Selected hosts: ${matrix.map(m => m.ssh_host_secret).join(', ') || '(none)'}`);
            core.setOutput('matrix', JSON.stringify(matrix));

  deploy:
    needs: build-matrix
    if: ${{ needs.build-matrix.outputs.matrix != '[]' }}
    runs-on: self-hosted

    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJSON(needs.build-matrix.outputs.matrix) }}

    # ensure at most one job per physical host at any time (across parallel runs)
    concurrency:
      group: asa-deploy-${{ matrix.ssh_host_secret }}
      cancel-in-progress: false

    env:
      SSH_USER: ${{ secrets.ASA_SERVER_SSH_USER }}
      SSH_PORT: ${{ secrets.ASA_SERVER_PORT }}
      HOST: ${{ secrets[matrix.ssh_host_secret] }}   # resolves the host IP/hostname for this job
      WORK_DIR: ${{ secrets.WORK_DIR }}

    steps:
      - name: Mint GitHub App token
        id: app
        uses: actions/create-github-app-token@v2
        with:
          app-id: ${{ secrets.APP_ID }}                 # org secrets
          private-key: ${{ secrets.APP_PRIVATE_KEY }}

      - name: Checkout (used to stage PR changes)
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ steps.app.outputs.token }}

      - name: Setup SSH
        uses: webfactory/ssh-agent@v0.9.1
        with:
          ssh-private-key: ${{ secrets.ASA_SERVERS_PRIVATE_KEY }}

      - name: Write target list for this host
        id: write-list
        uses: actions/github-script@v7
        env:
          TARGETS_JSON: ${{ toJSON(matrix.targets) }}
        with:
          script: |
            const fs = require('fs');
            const t = JSON.parse(process.env.TARGETS_JSON || '[]');
            // Write a simple pipe-delimited list we can loop over in bash
            const lines = t.map(x => [x.id, x.work_subdir, x.container, x.volume].join('|')).join('\n');
            fs.writeFileSync('targets.list', lines);
            core.info(`Targets for host:\n${lines}`);

      # --- Loop maps on this host sequentially ---
      - name: Deploy each changed map (drain → stop → update → start → readiness → sync GUS)
        id: loop
        shell: bash
        run: |
          set -euo pipefail

          # file to collect any GUS paths that changed (for PR)
          : > changed_gus_paths.txt

          while IFS='|' read -r ID SUBDIR CNAME VOLUME; do
            echo "::group::Processing ${ID} (${SUBDIR})"

            # --- 30m warning
            ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
              "sudo docker exec '$CNAME' asa-ctrl rcon --exec \"serverchat Server restarting in 30 minutes for updates. Get somewhere safe.\" || true"

            sleep "${WAIT_30M}"

            # --- 15m warning
            ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
              "sudo docker exec '$CNAME' asa-ctrl rcon --exec \"serverchat Server restarting in 15 minutes for updates. Get somewhere safe.\" || true"

            sleep "${WAIT_15M}"

            # --- 3m warning
            ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
              "sudo docker exec '$CNAME' asa-ctrl rcon --exec \"serverchat Server restarting in 3 minutes for updates. Final Warning get somewhere safe.\" || true"

            sleep "${WAIT_3M}"

            # --- Saveworld & announce
            ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
              "sudo docker exec '$CNAME' asa-ctrl rcon --exec \"serverchat Saving world and shutting down for updates.\" && \
               sudo docker exec '$CNAME' asa-ctrl rcon --exec \"saveworld\" || true"

            # --- Stop container
            ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
              "cd '$WORK_DIR/$SUBDIR' && sudo docker compose down"

            # --- Update host checkout (compose only)
            ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" <<'EOF'
            set -euo pipefail
            cd "${WORK_DIR}/${SUBDIR}"

            TOKEN='${{ steps.app.outputs.token }}'
            REPO_SLUG='${{ github.repository }}'
            REPO_URL="https://x-access-token:${TOKEN}@github.com/${REPO_SLUG}.git"

            GIT_TERMINAL_PROMPT=0 git fetch --no-tags --prune "$REPO_URL" main
            git reset --hard FETCH_HEAD
            EOF

            # --- Start container
            ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
              "cd '$WORK_DIR/$SUBDIR' && sudo docker compose up -d"

            # --- Wait for startup log
            ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" <<'EOF'
            set -euo pipefail
            CNAME='${CNAME}'
            end=$((SECONDS+${LOG_READY_TIMEOUT}))
            while ! sudo docker logs --tail 500 "$CNAME" 2>&1 | grep -q "Starting the ARK: Survival Ascended dedicated server"; do
              if [ $SECONDS -gt $end ]; then
                echo "Timed out waiting for startup log ($CNAME)" >&2
                sudo docker logs "$CNAME" --tail 200 || true
                exit 1
              fi
              sleep 5
            done
            EOF

            # --- Wait for RCON readiness
            ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" <<'EOF'
            set -euo pipefail
            CNAME='${CNAME}'
            end=$((SECONDS+${RCON_READY_TIMEOUT}))
            while :; do
              if sudo docker exec "$CNAME" asa-ctrl rcon --exec "listplayers" >/dev/null 2>&1 \
                 || sudo docker exec "$CNAME" asa-ctrl rcon --exec "serverchat healthcheck" >/dev/null 2>&1; then
                break
              fi
              if [ $SECONDS -gt $end ]; then
                echo "Timed out waiting for RCON ($CNAME)" >&2
                sudo docker logs "$CNAME" --tail 200 || true
                exit 1
              fi
              sleep 5
            done
            EOF

            # --- Fetch live GameUserSettings.ini (read-only)
            LIVE_TMP="live-${ID}"
            mkdir -p "$LIVE_TMP"
            ssh -o StrictHostKeyChecking=no -p "$SSH_PORT" "$SSH_USER@$HOST" \
              "sudo docker run --rm -v '${VOLUME}:/v:ro' alpine:3.19 \
               sh -c 'if [ -f /v/ShooterGame/Saved/Config/WindowsServer/GameUserSettings.ini ]; then cat /v/ShooterGame/Saved/Config/WindowsServer/GameUserSettings.ini; fi'" \
              > "$LIVE_TMP/GameUserSettings.ini" || true

            # scrub secret BEFORE staging to repo
            if [ -s "$LIVE_TMP/GameUserSettings.ini" ]; then
              sed -i 's|^ServerAdminPassword=.*$|ServerAdminPassword=\${SERVER_PASSWORD}|' "$LIVE_TMP/GameUserSettings.ini"
            fi

            # compare with repo copy; stage only if different
            REPO_INI_PATH="${{ github.workspace }}/${SUBDIR}/GameUserSettings.ini"
            if [ -s "$LIVE_TMP/GameUserSettings.ini" ]; then
              if ! diff -q "$REPO_INI_PATH" "$LIVE_TMP/GameUserSettings.ini" >/dev/null 2>&1; then
                cp "$LIVE_TMP/GameUserSettings.ini" "$REPO_INI_PATH"
                echo "${SUBDIR}/GameUserSettings.ini" >> changed_gus_paths.txt
                echo "Detected change in ${SUBDIR}/GameUserSettings.ini (staged for PR)"
              else
                echo "No changes in ${SUBDIR}/GameUserSettings.ini"
              fi
            else
              echo "Live GameUserSettings.ini missing for ${ID}; nothing to compare."
            fi

            echo "::endgroup::"
          done < targets.list

          # expose whether we have anything to PR
          CHANGED_COUNT=$(wc -l < changed_gus_paths.txt || echo 0)
          echo "changed_count=${CHANGED_COUNT}" >> "$GITHUB_OUTPUT"

      - name: Create PR with updated GameUserSettings.ini (all changed on this host)
        if: steps.loop.outputs.changed_count != '0'
        id: cpr
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ steps.app.outputs.token }}
          commit-message: "chore(${{
            join(fromJSON(steps.write-list.outputs.result || '[]'), ', ')
          }}): sync live GameUserSettings.ini -> repo"
          title: "Sync GameUserSettings.ini from live (${{
            matrix.ssh_host_secret
          }})"
          body: |
            Syncing `GameUserSettings.ini` from the live servers on **host group** `${{ matrix.ssh_host_secret }}` after restart.

            **Updated files:**
            ${{ steps.loop.outputs.changed_count }} file(s)
            ```
            ${{ steps.loop.outputs.changed_count && join(steps.loop.outputs, '') }}
            ```
          branch: chore/sync-gus/${{ matrix.ssh_host_secret }}-${{ github.run_id }}
          delete-branch: true
          add-paths: |
            ${{ steps.loop.outputs.changed_count && hashFiles('**/GameUserSettings.ini') && join(steps.loop.outputs, '') }}
          # The action reads changes from the working tree; we've already copied the changed files.

      - name: Summarize
        run: |
          if [ "${{ steps.loop.outputs.changed_count }}" != "0" ]; then
            echo "### GUS changed → PR opened" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "### No GUS changes detected → no PR" >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Kill ssh-agent (cleanup)
        if: always()
        run: |
          if [ -n "${SSH_AGENT_PID:-}" ]; then ssh-agent -k || true; fi
